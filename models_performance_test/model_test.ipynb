{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.mobienetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using device: cuda\n",
      "[1, 100] loss: 1.665\n",
      "[2, 100] loss: 0.737\n",
      "[3, 100] loss: 0.371\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # 将 28x28 调整为 224x224 适用于 MobileNetV2\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)), \n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "dataset_path = './data'\n",
    "train_dataset = datasets.MNIST(root=dataset_path, train=True, download=True, transform=transform)\n",
    "\n",
    "# 只使用部分训练数据（例如 1000 个样本）\n",
    "train_subset_size = 1000  # 只使用前 1000 个样本\n",
    "train_subset = torch.utils.data.Subset(train_dataset, range(train_subset_size))\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=2)\n",
    "# 加载 MobileNetV2 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = mobilenet_v2(weights=None)  # 不使用预训练权重\n",
    "model.classifier[1] = nn.Linear(model.last_channel, 10)  # MNIST 有 10 个类别\n",
    "model = model.to(device)\n",
    "print(f\"Model is using device: {device}\")\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'mobilenet_v2_mnist.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_487185/1848148051.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('mobilenet_v2_mnist.pth'))  # 加载之前保存的模型\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels:  [7 2 1 0 4 1 4 9 5 9]\n",
      "Predic labels:  [7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # 将 28x28 调整为 224x224 适用于 MobileNetV2\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)), \n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "dataset_path = './data'\n",
    "test_dataset = datasets.MNIST(root=dataset_path, train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# 加载 MobileNetV2 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = mobilenet_v2(weights=None)\n",
    "model.classifier[1] = nn.Linear(model.last_channel, 10)  # MNIST 有 10 个类别\n",
    "model.load_state_dict(torch.load('mobilenet_v2_mnist.pth'))  # 加载之前保存的模型\n",
    "model = model.to(device)\n",
    "\n",
    "# 使用测试数据进行推理\n",
    "model.eval()\n",
    "test_iter = iter(test_loader)  # 迭代器，用于取出测试数据集中的一批数据\n",
    "images, labels = next(test_iter)  # 取出一批数据\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():  # 不需要计算梯度\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# 打印实际标签和预测结果\n",
    "print(\"Actual labels: \", labels[:10].cpu().numpy())\n",
    "print(\"Predic labels: \", predicted[:10].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. inceptionv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fushaomin/miniconda3/lib/python3.12/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # 将 28x28 调整为 299x299 适用于 InceptionV3\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "train_dataset = torch.utils.data.Subset(datasets.MNIST(root='./data', train=True, download=False, transform=transform), range(1000))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 InceptionV3 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = inception_v3(weights=None)  # 不使用预训练权重\n",
    "model.AuxLogits.fc = nn.Linear(model.AuxLogits.fc.in_features, 10)  # MNIST 有 10 个类别\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs, aux_outputs = model(inputs)\n",
    "        loss1 = criterion(outputs, labels)\n",
    "        loss2 = criterion(aux_outputs, labels)\n",
    "        loss = loss1 + 0.4 * loss2  # InceptionV3 的损失函数结合了辅助分类器的输出\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'inceptionv3_mnist.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs, aux_outputs = model(inputs)\n",
    "        loss1 = criterion(outputs, labels)\n",
    "        loss2 = criterion(aux_outputs, labels)\n",
    "        loss = loss1 + 0.4 * loss2  # InceptionV3 的损失函数结合了辅助分类器的输出\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'inceptionv3_mnist.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/3753212424.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('inceptionv3_mnist.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 85.97%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # 将 28x28 调整为 299x299 适用于 InceptionV3\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# 加载 InceptionV3 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = inception_v3(weights=None)  # 不使用预训练权重\n",
    "model.AuxLogits.fc = nn.Linear(model.AuxLogits.fc.in_features, 10)  # MNIST 有 10 个类别\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model.load_state_dict(torch.load('inceptionv3_mnist.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 推理\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.inceptionv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretrainedmodels\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f218d715190>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f218d715580>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from pretrainedmodels) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from pretrainedmodels) (0.20.1)\n",
      "Collecting munch (from pretrainedmodels)\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: tqdm in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from pretrainedmodels) (4.66.5)\n",
      "Requirement already satisfied: filelock in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torch->pretrainedmodels) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torchvision->pretrainedmodels) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from torchvision->pretrainedmodels) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n",
      "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Building wheels for collected packages: pretrainedmodels\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=255f9130517ee94424cab8f930684269c2b08b24bb2dda5704f43d42f3eaefed\n",
      "  Stored in directory: /home/fushaomin/.cache/pip/wheels/4c/01/56/40a48f75dbdfe167a0cb70d3b48913369a00ec5c4e9fed5f2b\n",
      "Successfully built pretrainedmodels\n",
      "Installing collected packages: munch, pretrainedmodels\n",
      "Successfully installed munch-4.0.0 pretrainedmodels-0.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import inception_v3\n",
    "import pretrainedmodels\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # 将 28x28 调整为 299x299 适用于 InceptionV4\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "train_dataset = torch.utils.data.Subset(datasets.MNIST(root='./data', train=True, download=False, transform=transform), range(1000))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 InceptionV4 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = pretrainedmodels.__dict__['inceptionv4'](num_classes=1000, pretrained=None)  # 不使用预训练权重\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 10)  # MNIST 有 10 个类别\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'inceptionv4_mnist.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/1017547500.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('inceptionv4_mnist.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 66.31%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import pretrainedmodels\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # 将 28x28 调整为 299x299 适用于 InceptionV4\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# 加载 InceptionV4 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = pretrainedmodels.__dict__['inceptionv4'](num_classes=1000, pretrained=None)\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 10)  # 不使用预训练权重\n",
    "model.load_state_dict(torch.load('inceptionv4_mnist.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 推理\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inception-ResNet-V2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import pretrainedmodels\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # 将 28x28 调整为 299x299 适用于 Inception-ResNet-V2\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "train_dataset = torch.utils.data.Subset(datasets.MNIST(root='./data', train=True, download=False, transform=transform), range(1000))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 Inception-ResNet-V2 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = pretrainedmodels.__dict__['inceptionresnetv2'](num_classes=1000, pretrained=None)  # 不使用预训练权重\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 10)  # MNIST 有 10 个类别\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'inceptionresnetv2_mnist.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/3090169951.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('inceptionresnetv2_mnist.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 80.08%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import pretrainedmodels\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # 将 28x28 调整为 299x299 适用于 Inception-ResNet-V2\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "        \n",
    "# 加载 Inception-ResNet-V2 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = pretrainedmodels.__dict__['inceptionresnetv2'](num_classes=1000, pretrained=None)  # 不使用预训练权重\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 10)  # MNIST 有 10 个类别\n",
    "model.load_state_dict(torch.load('inceptionresnetv2_mnist.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 推理\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.ResNet-V2-50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fushaomin/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/fushaomin/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import pretrainedmodels\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将 28x28 调整为 224x224 适用于 ResNet-V2-50\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "train_dataset = torch.utils.data.Subset(datasets.MNIST(root='../data', train=True, download=False, transform=transform), range(1000))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 ResNet-V2-50 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = pretrainedmodels.__dict__['resnet50'](num_classes=1000, pretrained=None)  # 不使用预训练权重\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 10)  # MNIST 有 10 个类别\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'resnetv2_50_mnist.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/2897211374.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('resnetv2_50_mnist.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 36.41%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import pretrainedmodels\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将 28x28 调整为 224x224 适用于 ResNet-V2-50\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# 加载 ResNet-V2-50 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = pretrainedmodels.__dict__['resnet50'](num_classes=1000, pretrained=None)  # 不使用预训练权重\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 10)  # MNIST 有 10 个类别\n",
    "model.load_state_dict(torch.load('resnetv2_50_mnist.pth'))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# 推理\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.ResNet-V2-152 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import pretrainedmodels\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将 28x28 调整为 224x224 适用于 ResNet-V2-152\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "train_dataset = torch.utils.data.Subset(datasets.MNIST(root='../data', train=True, download=False, transform=transform), range(1001))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 ResNet-V2-152 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = pretrainedmodels.__dict__['resnet152'](num_classes=1000, pretrained=None)  # 不使用预训练权重\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 10)  # MNIST 有 10 个类别\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'resnetv2_152_mnist.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/1550530928.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('resnetv2_152_mnist.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 82.89%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import pretrainedmodels\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将 28x28 调整为 224x224 适用于 ResNet-V2-152\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# 加载 ResNet-V2-152 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = pretrainedmodels.__dict__['resnet152'](num_classes=1000, pretrained=None)  # 不使用预训练权重\n",
    "model.last_linear = nn.Linear(model.last_linear.in_features, 10)  # MNIST 有 10 个类别\n",
    "model.load_state_dict(torch.load('resnetv2_152_mnist.pth'))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# 推理\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.VGG-16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将 28x28 调整为 224x224 适用于 VGG-16\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "train_dataset = torch.utils.data.Subset(datasets.MNIST(root='../data', train=True, download=False, transform=transform), range(1000))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 VGG-16 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.vgg16(pretrained=False)  # 不使用预训练权重\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)  # MNIST 有 10 个类别\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'vgg16_mnist.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/1817449263.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('vgg16_mnist.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 76.19%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将 28x28 调整为 224x224 适用于 VGG-16\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 将单通道转换为三通道\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 数据集标准化参数\n",
    "])\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# 加载 VGG-16 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.vgg16(pretrained=False)\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)  # MNIST 有 10 个类别\n",
    "model.load_state_dict(torch.load('vgg16_mnist.pth'))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# 推理\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图片准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-resolution images generated successfully!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 生成低分辨率图像\n",
    "class LowResolutionGenerator:\n",
    "    def __init__(self, input_dir, output_dir):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def generate(self):\n",
    "        for img_file in os.listdir(self.input_dir):\n",
    "            if img_file.endswith('.jpg') or img_file.endswith('.png'):\n",
    "                img_path = os.path.join(self.input_dir, img_file)\n",
    "                high_res_image = Image.open(img_path).convert('L')  # 转换为灰度图像\n",
    "                low_res_image = high_res_image.resize(\n",
    "                    (high_res_image.width // 2, high_res_image.height // 2), Image.BICUBIC\n",
    "                )\n",
    "                low_res_image = low_res_image.resize(\n",
    "                    (high_res_image.width, high_res_image.height), Image.BICUBIC\n",
    "                )\n",
    "                low_res_path = os.path.join(self.output_dir, img_file)\n",
    "                low_res_image.save(low_res_path)\n",
    "\n",
    "# 使用示例\n",
    "input_directory = './data/Set5'  # 高分辨率图像输入目录\n",
    "output_directory = './data/Set5_LR'  # 低分辨率图像输出目录\n",
    "\n",
    "lr_generator = LowResolutionGenerator(input_directory, output_directory)\n",
    "lr_generator.generate()\n",
    "\n",
    "print('Low-resolution images generated successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为 Tensor\n",
    "])\n",
    "\n",
    "# 修改 Set5 数据集类，添加 resize 步骤\n",
    "class Set5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(lr_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_img_name = os.path.join(self.lr_dir, self.image_files[idx])\n",
    "        hr_img_name = os.path.join(self.hr_dir, self.image_files[idx])\n",
    "        low_res_image = Image.open(lr_img_name).convert('L')  # 转换为灰度图像\n",
    "        high_res_image = Image.open(hr_img_name).convert('L')  # 转换为灰度图像\n",
    "\n",
    "        # 调整图像大小为统一尺寸，例如 256x256\n",
    "        low_res_image = low_res_image.resize((256, 256), Image.BICUBIC)\n",
    "        high_res_image = high_res_image.resize((256, 256), Image.BICUBIC)\n",
    "\n",
    "        if self.transform:\n",
    "            high_res_image = self.transform(high_res_image)\n",
    "            low_res_image = self.transform(low_res_image)\n",
    "        return low_res_image, high_res_image  # 输入是低分辨率图像，目标是高分辨率图像\n",
    "\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = Set5Dataset(lr_dir='../data/Set5_LR', hr_dir='../data/Set5', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# 定义 SRCNN 模型\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# 加载 SRCNN 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SRCNN().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(train_loader, 0):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)  # 使用输入图像和输出图像之间的均方误差作为损失\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # 每 10 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'srcnn_set5.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/1845509028.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('srcnn_set5.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image: 5.jpg, MSE: 0.0221\n",
      "Processed image: 1.jpg, MSE: 0.0073\n",
      "Processed image: 3.jpg, MSE: 0.0012\n",
      "Processed image: 4.jpg, MSE: 0.0022\n",
      "Processed image: 2.jpg, MSE: 0.0155\n",
      "Accuracy of the network on the test images: 60.00%\n",
      "Finished Inference\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # 将图像缩小为1000x1000像素\n",
    "    transforms.ToTensor(),  # 转换为 Tensor\n",
    "])\n",
    "\n",
    "# 定义 SRCNN 模型\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# 加载模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SRCNN().to(device)\n",
    "model.load_state_dict(torch.load('srcnn_set5.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 加载测试图像并进行推理\n",
    "test_dir = '../data/Set5_LR'  # 低分辨率图像文件夹\n",
    "test_images = [f for f in os.listdir(test_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for img_name in test_images:\n",
    "    lr_img_path = os.path.join(test_dir, img_name)\n",
    "    low_res_image = Image.open(lr_img_path).convert('L')  # 转换为灰度图像\n",
    "    input_tensor = transform(low_res_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # 推理\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    # 计算准确率（这里假设有高分辨率的 ground truth）\n",
    "    hr_img_path = os.path.join('../data/Set5', img_name)\n",
    "    high_res_image = transform(Image.open(hr_img_path).convert('L')).unsqueeze(0).to(device)\n",
    "\n",
    "    # 简单计算准确率（以 MSE 作为衡量标准，越小越好）\n",
    "    mse = nn.functional.mse_loss(output, high_res_image).item()\n",
    "    if mse < 0.01:  # 假设一个阈值来判断是否正确预测\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "    # 显示预测结果\n",
    "    print(f'Processed image: {img_name}, MSE: {mse:.4f}')\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "print('Finished Inference')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.VGG-19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像调整为适合 VGG19 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用 ImageNet 的均值和标准差进行标准化\n",
    "])\n",
    "\n",
    "# 自定义数据集加载器\n",
    "class Set5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_img_name = os.path.join(self.lr_dir, self.image_names[idx])\n",
    "        hr_img_name = os.path.join(self.hr_dir, self.image_names[idx])\n",
    "        lr_image = Image.open(lr_img_name).convert('RGB')\n",
    "        hr_image = Image.open(hr_img_name).convert('RGB')\n",
    "        if self.transform:\n",
    "            lr_image = self.transform(lr_image)\n",
    "            hr_image = self.transform(hr_image)\n",
    "        return lr_image, hr_image\n",
    "\n",
    "# 加载数据集（使用 Set5 数据集的低分辨率和高清图像）\n",
    "train_dataset = Set5Dataset(lr_dir='../data/Set5_LR', hr_dir='../data/Set5', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 VGG19 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.vgg19(pretrained=False)  # 不使用预训练权重\n",
    "model.classifier[6] = nn.Linear(4096, 3 * 224 * 224)  # 输出大小与高清图像一致\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view_as(labels)  # 调整输出形状与标签一致\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 batch 打印一次 loss\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'vgg19_set5.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/1953801695.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('vgg19_set5.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss for ./data/Set5_LR/5.jpg: 3.622990369796753\n",
      "MSE Loss for ./data/Set5_LR/1.jpg: 0.9915202260017395\n",
      "MSE Loss for ./data/Set5_LR/3.jpg: 1.3173009157180786\n",
      "MSE Loss for ./data/Set5_LR/4.jpg: 2.0026438236236572\n",
      "MSE Loss for ./data/Set5_LR/2.jpg: 0.8065729141235352\n",
      "Finished Inference\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像调整为适合 VGG19 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用 ImageNet 的均值和标准差进行标准化\n",
    "])\n",
    "\n",
    "# 加载 VGG19 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.vgg19(pretrained=False)\n",
    "model.classifier[6] = nn.Linear(4096, 3 * 224 * 224)\n",
    "model = model.to(device)\n",
    "\n",
    "# 加载训练好的模型权重\n",
    "model.load_state_dict(torch.load('vgg19_set5.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 推理函数\n",
    "def infer_image(lr_image_path, hr_image_path):\n",
    "    lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "    hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "    input_tensor = transform(lr_image).unsqueeze(0).to(device)\n",
    "    hr_tensor = transform(hr_image).unsqueeze(0).to(device)  # 确保高分辨率图像也在相同的设备上\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        output = output.view(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # 计算 MSE 作为准确度的指标之一\n",
    "    mse_loss = nn.MSELoss()(output, hr_tensor)\n",
    "    print(f\"MSE Loss for {lr_image_path}: {mse_loss.item()}\")\n",
    "\n",
    "# 推理数据集中的每张图像\n",
    "lr_dir = '../data/Set5_LR'\n",
    "hr_dir = '../data/Set5'\n",
    "image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "\n",
    "for img_name in image_names:\n",
    "    lr_image_path = os.path.join(lr_dir, img_name)\n",
    "    hr_image_path = os.path.join(hr_dir, img_name)\n",
    "    infer_image(lr_image_path, hr_image_path)\n",
    "\n",
    "print('Finished Inference')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.ResNet-SRGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定义数据集类\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        lr_image_path = os.path.join(self.lr_dir, img_name)\n",
    "        hr_image_path = os.path.join(self.hr_dir, img_name)\n",
    "        lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "        hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            lr_image = self.transform(lr_image)\n",
    "            hr_image = self.transform(hr_image)\n",
    "\n",
    "        return lr_image, hr_image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像调整为适合 ResNet 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用 ImageNet 的均值和标准差进行标准化\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "lr_dir = '../data/Set5_LR'\n",
    "hr_dir = '../data/Set5'\n",
    "dataset = ImageDataset(lr_dir, hr_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 ResNet 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 3 * 224 * 224)  # 输出与高分辨率图像大小匹配\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (lr_images, hr_images) in enumerate(train_loader, 0):\n",
    "        lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(lr_images)\n",
    "        outputs = outputs.view(-1, 3, 224, 224)  # 将输出重塑为图像大小\n",
    "        loss = criterion(outputs, hr_images)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # 每 10 个 batch 打印一次 loss\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'resnet_srgan_set5.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/2433598113.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('resnet_srgan_set5.pth'))  # 加载训练好的模型权重\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss for ./data/Set5_LR/5.jpg: 5890.63671875\n",
      "MSE Loss for ./data/Set5_LR/1.jpg: 233.42489624023438\n",
      "MSE Loss for ./data/Set5_LR/3.jpg: 605.3394165039062\n",
      "MSE Loss for ./data/Set5_LR/4.jpg: 3221.66943359375\n",
      "MSE Loss for ./data/Set5_LR/2.jpg: 92.18882751464844\n",
      "Finished Inference\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像调整为适合 ResNet-SRGAN 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用 ImageNet 的均值和标准差进行标准化\n",
    "])\n",
    "\n",
    "# 加载 ResNet-SRGAN 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 3 * 224 * 224)  # 输出与高分辨率图像大小匹配\n",
    "model.load_state_dict(torch.load('resnet_srgan_set5.pth'))  # 加载训练好的模型权重\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 推理函数\n",
    "def infer_image(lr_image_path, hr_image_path):\n",
    "    lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "    hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "    input_tensor = transform(lr_image).unsqueeze(0).to(device)\n",
    "    hr_tensor = transform(hr_image).unsqueeze(0).to(device)  # 确保高分辨率图像也在相同的设备上\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        output = output.view(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # 计算 MSE 作为准确度的指标之一\n",
    "    mse_loss = nn.MSELoss()(output, hr_tensor)\n",
    "    print(f\"MSE Loss for {lr_image_path}: {mse_loss.item()}\")\n",
    "\n",
    "# 推理数据集中的每张图像\n",
    "lr_dir = '../data/Set5_LR'\n",
    "hr_dir = '../data/Set5'\n",
    "image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "\n",
    "for img_name in image_names:\n",
    "    lr_image_path = os.path.join(lr_dir, img_name)\n",
    "    hr_image_path = os.path.join(hr_dir, img_name)\n",
    "    infer_image(lr_image_path, hr_image_path)\n",
    "\n",
    "print('Finished Inference')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.ResNet-DPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定义数据集类\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        lr_image_path = os.path.join(self.lr_dir, img_name)\n",
    "        hr_image_path = os.path.join(self.hr_dir, img_name)\n",
    "        lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "        hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            lr_image = self.transform(lr_image)\n",
    "            hr_image = self.transform(hr_image)\n",
    "\n",
    "        return lr_image, hr_image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像调整为适合 ResNet-DPED 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用 ImageNet 的均值和标准差进行标准化\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "lr_dir = '../data/Set5_LR'\n",
    "hr_dir = '../data/Set5'\n",
    "dataset = ImageDataset(lr_dir, hr_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 ResNet-DPED 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 3 * 224 * 224)  # 输出与高分辨率图像大小匹配\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (lr_images, hr_images) in enumerate(train_loader, 0):\n",
    "        lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(lr_images)\n",
    "        outputs = outputs.view(-1, 3, 224, 224)  # 将输出重塑为图像大小\n",
    "        loss = criterion(outputs, hr_images)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # 每 10 个 batch 打印一次 loss\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'resnet_dped_set5.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/2109442604.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('resnet_dped_set5.pth'))  # 加载训练好的模型权重\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss for ./data/Set5_LR/5.jpg: 92389.9921875\n",
      "MSE Loss for ./data/Set5_LR/1.jpg: 338.0931091308594\n",
      "MSE Loss for ./data/Set5_LR/3.jpg: 300.0672607421875\n",
      "MSE Loss for ./data/Set5_LR/4.jpg: 1889.5848388671875\n",
      "MSE Loss for ./data/Set5_LR/2.jpg: 8488.494140625\n",
      "Finished Inference\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 将图像调整为适合 ResNet-DPED 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用 ImageNet 的均值和标准差进行标准化\n",
    "])\n",
    "\n",
    "# 加载 ResNet-DPED 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 3 * 224 * 224)  # 输出与高分辨率图像大小匹配\n",
    "model.load_state_dict(torch.load('resnet_dped_set5.pth'))  # 加载训练好的模型权重\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 推理函数\n",
    "def infer_image(lr_image_path, hr_image_path):\n",
    "    lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "    hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "    input_tensor = transform(lr_image).unsqueeze(0).to(device)\n",
    "    hr_tensor = transform(hr_image).unsqueeze(0).to(device)  # 确保高分辨率图像也在相同的设备上\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        output = output.view(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # 计算 MSE 作为准确度的指标之一\n",
    "    mse_loss = nn.MSELoss()(output, hr_tensor)\n",
    "    print(f\"MSE Loss for {lr_image_path}: {mse_loss.item()}\")\n",
    "\n",
    "# 推理数据集中的每张图像\n",
    "lr_dir = '../data/Set5_LR'\n",
    "hr_dir = '../data/Set5'\n",
    "image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "\n",
    "for img_name in image_names:\n",
    "    lr_image_path = os.path.join(lr_dir, img_name)\n",
    "    hr_image_path = os.path.join(hr_dir, img_name)\n",
    "    infer_image(lr_image_path, hr_image_path)\n",
    "\n",
    "print('Finished Inference')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.U-Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# 定义 U-Net 模型\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # 定义编码器部分\n",
    "        self.encoder1 = self.conv_block(3, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "\n",
    "        # 定义中间层\n",
    "        self.middle = self.conv_block(512, 1024)\n",
    "\n",
    "        # 定义解码器部分\n",
    "        self.decoder4 = self.conv_block(1024 + 512, 512)\n",
    "        self.decoder3 = self.conv_block(512 + 256, 256)\n",
    "        self.decoder2 = self.conv_block(256 + 128, 128)\n",
    "        self.decoder1 = self.conv_block(128 + 64, 64)\n",
    "\n",
    "        # 定义输出层\n",
    "        self.output_layer = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 编码器部分\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.downsample(enc1))\n",
    "        enc3 = self.encoder3(self.downsample(enc2))\n",
    "        enc4 = self.encoder4(self.downsample(enc3))\n",
    "\n",
    "        # 中间层\n",
    "        middle = self.middle(self.downsample(enc4))\n",
    "\n",
    "        # 解码器部分\n",
    "        dec4 = self.upsample(middle, enc4)\n",
    "        dec4 = self.decoder4(torch.cat([dec4, enc4], dim=1))\n",
    "        dec3 = self.upsample(dec4, enc3)\n",
    "        dec3 = self.decoder3(torch.cat([dec3, enc3], dim=1))\n",
    "        dec2 = self.upsample(dec3, enc2)\n",
    "        dec2 = self.decoder2(torch.cat([dec2, enc2], dim=1))\n",
    "        dec1 = self.upsample(dec2, enc1)\n",
    "        dec1 = self.decoder1(torch.cat([dec1, enc1], dim=1))\n",
    "\n",
    "        # 输出层\n",
    "        return self.output_layer(dec1)\n",
    "\n",
    "    def downsample(self, x):\n",
    "        return nn.MaxPool2d(kernel_size=2, stride=2)(x)\n",
    "\n",
    "    def upsample(self, x, target_feature_map):\n",
    "        return nn.functional.interpolate(x, size=target_feature_map.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "# 定义数据集类\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        lr_image_path = os.path.join(self.lr_dir, img_name)\n",
    "        hr_image_path = os.path.join(self.hr_dir, img_name)\n",
    "        lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "        hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            lr_image = self.transform(lr_image)\n",
    "            hr_image = self.transform(hr_image)\n",
    "\n",
    "        return lr_image, hr_image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 将图像调整为适合 U-Net 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "lr_dir = '../data/Set5_LR'\n",
    "hr_dir = '../data/Set5'\n",
    "dataset = ImageDataset(lr_dir, hr_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "# 加载 U-Net 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (lr_images, hr_images) in enumerate(train_loader, 0):\n",
    "        lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(lr_images)\n",
    "        loss = criterion(outputs, hr_images)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # 每 10 个 batch 打印一次 loss\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 10:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(model.state_dict(), 'unet_set5.pth')  # 保存模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507652/260642840.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('unet_set5.pth'))  # 加载训练好的模型权重\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss for ./data/Set5_LR/5.jpg: 0.46087774634361267\n",
      "MSE Loss for ./data/Set5_LR/1.jpg: 0.524380624294281\n",
      "MSE Loss for ./data/Set5_LR/3.jpg: 0.6777042150497437\n",
      "MSE Loss for ./data/Set5_LR/4.jpg: 0.8753786087036133\n",
      "MSE Loss for ./data/Set5_LR/2.jpg: 0.1537054032087326\n",
      "Finished Inference\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 将图像调整为适合 U-Net 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 加载 U-Net 模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load('unet_set5.pth'))  # 加载训练好的模型权重\n",
    "model.eval()\n",
    "\n",
    "# 推理函数\n",
    "def infer_image(lr_image_path, hr_image_path):\n",
    "    lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "    hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "    input_tensor = transform(lr_image).unsqueeze(0).to(device)\n",
    "    hr_tensor = transform(hr_image).unsqueeze(0).to(device)  # 确保高分辨率图像也在相同的设备上\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    # 计算 MSE 作为准确度的指标之一\n",
    "    mse_loss = nn.MSELoss()(output, hr_tensor)\n",
    "    print(f\"MSE Loss for {lr_image_path}: {mse_loss.item()}\")\n",
    "\n",
    "# 推理数据集中的每张图像\n",
    "lr_dir = '../data/Set5_LR'\n",
    "hr_dir = '../data/Set5'\n",
    "image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "\n",
    "for img_name in image_names:\n",
    "    lr_image_path = os.path.join(lr_dir, img_name)\n",
    "    hr_image_path = os.path.join(hr_dir, img_name)\n",
    "    infer_image(lr_image_path, hr_image_path)\n",
    "\n",
    "print('Finished Inference')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.Nvidia-SPADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainOptions\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter_counter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterationCounter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'options'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from options.train_options import TrainOptions\n",
    "import data\n",
    "from util.iter_counter import IterationCounter\n",
    "from util.visualizer import Visualizer\n",
    "from trainers.pix2pix_trainer import Pix2PixTrainer\n",
    "\n",
    "# 定义数据集类\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.image_names = [f for f in os.listdir(lr_dir) if os.path.isfile(os.path.join(lr_dir, f))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        lr_image_path = os.path.join(self.lr_dir, img_name)\n",
    "        hr_image_path = os.path.join(self.hr_dir, img_name)\n",
    "        lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "        hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            lr_image = self.transform(lr_image)\n",
    "            hr_image = self.transform(hr_image)\n",
    "\n",
    "        return lr_image, hr_image\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 将图像调整为适合 SPADE 的输入大小\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "lr_dir = './data/Set5_LR'\n",
    "hr_dir = './data/Set5'\n",
    "dataset = ImageDataset(lr_dir, hr_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "# parse options\n",
    "opt = TrainOptions().parse()\n",
    "\n",
    "# print options to help debugging\n",
    "print(' '.join(sys.argv))\n",
    "\n",
    "# create trainer for our model\n",
    "trainer = Pix2PixTrainer(opt)\n",
    "\n",
    "# create tool for counting iterations\n",
    "iter_counter = IterationCounter(opt, len(train_loader))\n",
    "\n",
    "# create tool for visualization\n",
    "visualizer = Visualizer(opt)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in iter_counter.training_epochs():\n",
    "    iter_counter.record_epoch_start(epoch)\n",
    "    for i, data_i in enumerate(train_loader, start=iter_counter.epoch_iter):\n",
    "        iter_counter.record_one_iteration()\n",
    "\n",
    "        # Training\n",
    "        # train generator\n",
    "        if i % opt.D_steps_per_G == 0:\n",
    "            trainer.run_generator_one_step(data_i)\n",
    "\n",
    "        # train discriminator\n",
    "        trainer.run_discriminator_one_step(data_i)\n",
    "\n",
    "        # Visualizations\n",
    "        if iter_counter.needs_printing() and i % 10 == 9:\n",
    "            losses = trainer.get_latest_losses()\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] loss: {losses}\")\n",
    "            visualizer.print_current_errors(epoch, iter_counter.epoch_iter,\n",
    "                                            losses, iter_counter.time_per_iter)\n",
    "            visualizer.plot_current_errors(losses, iter_counter.total_steps_so_far)\n",
    "\n",
    "        if iter_counter.needs_displaying():\n",
    "            visuals = OrderedDict([('input_label', data_i[0]),\n",
    "                                   ('synthesized_image', trainer.get_latest_generated()),\n",
    "                                   ('real_image', data_i[1])])\n",
    "            visualizer.display_current_results(visuals, epoch, iter_counter.total_steps_so_far)\n",
    "\n",
    "        if iter_counter.needs_saving():\n",
    "            print('saving the latest model (epoch %d, total_steps %d)' %\n",
    "                  (epoch, iter_counter.total_steps_so_far))\n",
    "            trainer.save('latest')\n",
    "            iter_counter.record_current_iter()\n",
    "\n",
    "    trainer.update_learning_rate(epoch)\n",
    "    iter_counter.record_epoch_end()\n",
    "\n",
    "    if epoch % opt.save_epoch_freq == 0 or \\\n",
    "       epoch == iter_counter.total_epochs:\n",
    "        print('saving the model at the end of epoch %d, iters %d' %\n",
    "              (epoch, iter_counter.total_steps_so_far))\n",
    "        trainer.save('latest')\n",
    "        trainer.save(epoch)\n",
    "\n",
    "print('Training was successfully finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./data/VOCtrainval_11-May-2012.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 295k/2.00G [00:06<12:05:30, 45.9kB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),  \u001b[38;5;66;03m# 缩小图像加快训练\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      9\u001b[0m ])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 加载 PASCAL VOC 数据集\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m VOCSegmentation(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, year\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2012\u001b[39m\u001b[38;5;124m'\u001b[39m, image_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     13\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m VOCSegmentation(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, year\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2012\u001b[39m\u001b[38;5;124m'\u001b[39m, image_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/datasets/voc.py:98\u001b[0m, in \u001b[0;36m_VOCBase.__init__\u001b[0;34m(self, root, year, image_set, download, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     95\u001b[0m voc_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, base_dir)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 98\u001b[0m     download_and_extract_archive(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmd5)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(voc_root):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/datasets/utils.py:395\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    393\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 395\u001b[0m download_url(url, download_root, filename, md5)\n\u001b[1;32m    397\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/datasets/utils.py:132\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[0;32m--> 132\u001b[0m     _urlretrieve(url, fpath)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/datasets/utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m response\u001b[38;5;241m.\u001b[39mread(chunk_size):\n\u001b[1;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 数据集\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms\n",
    "\n",
    "# 数据增强与预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # 缩小图像加快训练\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载 PASCAL VOC 数据集\n",
    "train_dataset = VOCSegmentation(root='./data', year='2012', image_set='train', download=True, transform=transform)\n",
    "val_dataset = VOCSegmentation(root='./data', year='2012', image_set='val', download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated train.txt with 155 valid images.\n",
      "Updated val.txt with 175 valid images.\n",
      "Updated test.txt with 210 valid images.\n"
     ]
    }
   ],
   "source": [
    "# 图片清理\n",
    "\n",
    "import os\n",
    "\n",
    "def clean_dataset(voc_dir):\n",
    "    annotations_dir = os.path.join(voc_dir, 'Annotations')\n",
    "    images_dir = os.path.join(voc_dir, 'JPEGImages')\n",
    "    imagesets_dir = os.path.join(voc_dir, 'ImageSets', 'Segmentation')\n",
    "\n",
    "    # 1. 清理缺少图片的标注文件\n",
    "    annotations = set(os.path.splitext(f)[0] for f in os.listdir(annotations_dir))\n",
    "    images = set(os.path.splitext(f)[0] for f in os.listdir(images_dir))\n",
    "    missing_images = annotations - images\n",
    "    for missing in missing_images:\n",
    "        os.remove(os.path.join(annotations_dir, f\"{missing}.xml\"))\n",
    "        print(f\"Deleted annotation: {missing}.xml\")\n",
    "\n",
    "    # 2. 更新 ImageSets 列表\n",
    "    for split in ['train.txt', 'val.txt', 'test.txt']:\n",
    "        split_file = os.path.join(imagesets_dir, split)\n",
    "        if not os.path.exists(split_file):\n",
    "            continue\n",
    "        with open(split_file, 'r') as f:\n",
    "            image_ids = [line.strip() for line in f]\n",
    "        valid_ids = [img_id for img_id in image_ids if img_id in images]\n",
    "        with open(split_file, 'w') as f:\n",
    "            f.write('\\n'.join(valid_ids))\n",
    "            print(f\"Updated {split} with {len(valid_ids)} valid images.\")\n",
    "\n",
    "# 执行清理\n",
    "clean_dataset('./data/VOCdevkit/VOC2007')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Annotations: set()\n",
      "Missing Images: set()\n",
      "Missing images from ImageSets: []\n"
     ]
    }
   ],
   "source": [
    "# VOC2007数据集验证代码\n",
    "import os\n",
    "\n",
    "# 路径设置\n",
    "data_dir = \"./data/VOCdevkit/VOC2007\"\n",
    "annotations_dir = os.path.join(data_dir, \"Annotations\")\n",
    "images_dir = os.path.join(data_dir, \"JPEGImages\")\n",
    "\n",
    "# 验证是否所有标注文件都有对应的图片\n",
    "annotation_files = set(os.path.splitext(f)[0] for f in os.listdir(annotations_dir))\n",
    "image_files = set(os.path.splitext(f)[0] for f in os.listdir(images_dir))\n",
    "\n",
    "missing_annotations = image_files - annotation_files\n",
    "missing_images = annotation_files - image_files\n",
    "\n",
    "print(\"Missing Annotations:\", missing_annotations)\n",
    "print(\"Missing Images:\", missing_images)\n",
    "# 验证 ImageSets/Segmentation 中的文件是否存在于 JPEGImages 中\n",
    "imageset_file = os.path.join(data_dir, \"ImageSets/Segmentation/train.txt\")\n",
    "with open(imageset_file, \"r\") as f:\n",
    "    image_ids = [line.strip() for line in f]\n",
    "\n",
    "missing_images = [img_id for img_id in image_ids if not os.path.exists(os.path.join(images_dir, f\"{img_id}.jpg\"))]\n",
    "print(\"Missing images from ImageSets:\", missing_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure is correct!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "voc_root = './data/VOCdevkit/VOC2007'\n",
    "annotations_dir = os.path.join(voc_root, 'Annotations')\n",
    "images_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "imagesets_dir = os.path.join(voc_root, 'ImageSets', 'Segmentation')\n",
    "\n",
    "# 检查核心目录是否存在\n",
    "assert os.path.isdir(annotations_dir), \"Annotations folder is missing!\"\n",
    "assert os.path.isdir(images_dir), \"JPEGImages folder is missing!\"\n",
    "assert os.path.isdir(imagesets_dir), \"ImageSets/Segmentation folder is missing!\"\n",
    "\n",
    "# 检查关键文件\n",
    "train_file = os.path.join(imagesets_dir, 'train.txt')\n",
    "val_file = os.path.join(imagesets_dir, 'val.txt')\n",
    "assert os.path.isfile(train_file), \"train.txt is missing!\"\n",
    "assert os.path.isfile(val_file), \"val.txt is missing!\"\n",
    "\n",
    "print(\"Dataset structure is correct!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 valid images with masks.\n",
      "Missing masks: 155\n",
      "Updated ./data/VOCdevkit/VOC2007/ImageSets/Segmentation/train.txt with valid entries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 数据路径\n",
    "segmentation_dir = './data/VOCdevkit/VOC2007/SegmentationClass'\n",
    "images_dir = './data/VOCdevkit/VOC2007/JPEGImages'\n",
    "imagesets_file = './data/VOCdevkit/VOC2007/ImageSets/Segmentation/train.txt'\n",
    "\n",
    "# 加载 train.txt 文件\n",
    "with open(imagesets_file, 'r') as f:\n",
    "    image_ids = [line.strip() for line in f]\n",
    "\n",
    "# 筛选有对应 Mask 文件的图片\n",
    "valid_ids = [img_id for img_id in image_ids if os.path.exists(os.path.join(segmentation_dir, f\"{img_id}.png\"))]\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Found {len(valid_ids)} valid images with masks.\")\n",
    "print(f\"Missing masks: {len(image_ids) - len(valid_ids)}\")\n",
    "\n",
    "# 更新 train.txt 文件（可选）\n",
    "with open(imagesets_file, 'w') as f:\n",
    "    f.write('\\n'.join(valid_ids))\n",
    "    print(f\"Updated {imagesets_file} with valid entries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.|CNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICNet(\n",
      "  (conv): Conv2d(3, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  4, 14, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  4,  6,  9, 10, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  7,  8, 12, 13, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  6, 15, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8, 12, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 10, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1, 11, 15, 16, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 12, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 12, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  9, 11, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 14, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 12, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  9, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  4,  9, 14, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  7,  9, 12, 14, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  7, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2, 15, 18, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  7,  8,  9, 15, 16, 17, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  6, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  6, 11, 12, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 10, 11, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 12, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  8,  9, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  8,  9, 11, 12, 13, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 13, 15, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  7, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  4,  6,  7, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  9, 11, 12, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  6,  7], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  7,  8, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  6,  7, 13, 15, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  5, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6, 11, 12, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 12, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7, 13, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 11, 14, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  5, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  7,  8,  9, 10, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 10, 13, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  9, 11, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  8, 12, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  9, 12, 15, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  8,  9, 11, 12, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7, 11, 15, 17, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  8,  9, 13, 15, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  5, 14, 15, 16, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  8, 10, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 11, 12, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  9, 15, 16, 18, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  6,  9, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([1, 3, 128, 128])\n",
      "Masks shape: torch.Size([1, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 10], device='cuda:0')\n",
      "Model output shape: torch.Size([1, 21, 128, 128])\n",
      "Epoch [1/5], Loss: 2.5843920887641185\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 10, 11, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  8,  9, 13, 15, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  6,  7, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2, 13, 14, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7,  8,  9, 11, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 14, 15, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  9, 11, 12, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 11, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  8, 12, 14, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  7,  8,  9, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  6,  7,  9, 15, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  6,  9, 11, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1, 12, 13, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  7, 10, 13, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8, 12, 13], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 11, 12, 15, 17, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 12, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  6, 12, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  4, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  4, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 14, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 10, 12, 13, 14, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  6,  7,  8, 14, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  6,  7, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 10, 12, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 15, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  9, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7,  8, 13, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  6, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8, 15, 16, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  4,  7,  8, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  5, 10, 13, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 10, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  7,  9, 10, 11, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  9, 11, 13, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  6, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 11, 14, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  9, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  5, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  6, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 11, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 10, 13, 15, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 12, 13, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  9, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  4,  6, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1, 14, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  7, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  8,  9, 12, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 10, 12, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 13, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([1, 3, 128, 128])\n",
      "Masks shape: torch.Size([1, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 11, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([1, 21, 128, 128])\n",
      "Epoch [2/5], Loss: 2.043905291917189\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 15, 16, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  7,  8,  9, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 12, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  8,  9, 11, 12, 13, 15, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  9, 17, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  6, 10, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  7,  8, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  4, 12, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  8,  9, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  5, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 12, 15, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 11, 15, 17, 18, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  9, 11, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  8, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  6,  8,  9, 13, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 16, 18, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  3,  4,  7,  9, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 12, 14, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 12, 14, 15, 16, 17, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3, 10, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  5, 15, 17, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  8, 12], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 14, 15, 17, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  7, 14, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 13, 14, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  7, 10, 12, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  7, 10, 11, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  6,  8, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6, 11, 12, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  9, 10, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  5,  7,  9, 10, 11, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  6,  7, 11, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  7,  9, 11, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 13, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8, 13, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 11, 12, 15, 16, 18, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  3,  6,  7, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  8,  9, 11, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  4, 10, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 10, 11, 13, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  7, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 10, 11, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  6, 13, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 12, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1, 13, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  4, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 13, 14, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  8, 12, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 15, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  9, 11, 13, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([1, 3, 128, 128])\n",
      "Masks shape: torch.Size([1, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([1, 21, 128, 128])\n",
      "Epoch [3/5], Loss: 1.775390310107537\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  9, 12, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 10, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  8,  9, 12, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  9, 12, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 11, 15, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  6,  7,  9, 15, 16, 18, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 15, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  9, 10, 11, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 11, 12, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 12, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1, 10, 13, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  6,  8, 11, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  6, 11, 12, 14], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  8,  9, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 10, 12, 13, 14, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  6, 17, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  4,  5, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  8,  9, 10, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7,  9, 12, 13, 14, 15, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  3,  6,  7, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  8,  9, 11, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  9, 10, 15, 18, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  8, 12, 14, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7,  9, 13, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  6,  7,  9, 11, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  7,  9, 13, 15, 16, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  7,  8, 12, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  4, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  9, 11, 15, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4, 10, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3, 12, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 11, 14, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  6,  7,  9, 11, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 10, 13, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  6,  7, 10, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 12, 14, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 12, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 12, 15, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  8, 13, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1, 12, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  5,  8, 11, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  6,  7, 14, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  8,  9, 10, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  8, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  8,  9, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  8,  9, 17, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7, 11, 13, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  5,  7, 11, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3, 13, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([1, 3, 128, 128])\n",
      "Masks shape: torch.Size([1, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([1, 21, 128, 128])\n",
      "Epoch [4/5], Loss: 1.6575139086201507\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  5, 11, 12, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  8, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 12, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  4, 11, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  5,  7,  9, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  5, 10, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  4, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5, 11, 13, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  5,  9, 15, 17, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7, 12, 13, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  9, 15, 18, 19, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 13, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  7, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 12, 13, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  8, 14, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  7, 13, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7,  8,  9, 11, 12, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2, 10, 15, 17, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  8,  9, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 12, 15, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  9, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 12, 14, 15, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  6,  9, 11, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 13, 15, 16, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  8,  9, 10, 12, 13, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4,  5,  9, 12, 15, 16, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 11, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 12, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7,  9, 10, 15, 16, 17, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  6, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  8, 10, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  6,  9, 11, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 15, 16, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  8, 17, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3, 11, 14, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  9, 12, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6,  7,  9, 11, 15, 16, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 11, 13, 14, 15, 16], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  5,  8, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0, 12, 14, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8, 10, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  5,  7, 12, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  5, 14, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  6, 12, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 11, 12, 13, 15], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  4, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  5,  6,  7,  9, 11, 15, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  8, 10, 14, 15, 17], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  6, 10, 13, 14, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  7, 10, 12, 15, 18], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  7, 10, 13, 15, 19], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([4, 3, 128, 128])\n",
      "Masks shape: torch.Size([4, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  2,  9, 10, 13, 15, 16, 18, 20], device='cuda:0')\n",
      "Model output shape: torch.Size([4, 21, 128, 128])\n",
      "Images shape: torch.Size([1, 3, 128, 128])\n",
      "Masks shape: torch.Size([1, 128, 128])\n",
      "Unique mask values: tensor([-1,  0,  4], device='cuda:0')\n",
      "Model output shape: torch.Size([1, 21, 128, 128])\n",
      "Epoch [5/5], Loss: 1.607213483666474\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as F_transforms\n",
    "\n",
    "# 定义 ICNet 模型\n",
    "class ICNet(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        super(ICNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return nn.functional.interpolate(x, size=(128, 128), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# 数据加载器中的标签处理\n",
    "def mask_transform(mask):\n",
    "    mask = mask.resize((128, 128))\n",
    "    mask = F_transforms.pil_to_tensor(mask).squeeze(0)\n",
    "    mask = mask.long()\n",
    "\n",
    "    # 将超出范围的值标记为无效值\n",
    "    mask[(mask < 0) | (mask >= 21)] = -1\n",
    "    return mask\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 数据加载与预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = VOCSegmentation(\n",
    "        root='../data',\n",
    "        year='2007',\n",
    "        image_set='train',\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=mask_transform\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    # 模型定义\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ICNet(num_classes=21).to(device)\n",
    "    print(model)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(5):  # 假设训练 5 个 epoch\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (images, masks) in enumerate(data_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # 打印调试信息\n",
    "            print(\"Images shape:\", images.shape)\n",
    "            print(\"Masks shape:\", masks.shape)\n",
    "            print(\"Unique mask values:\", torch.unique(masks))\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(images)\n",
    "            print(\"Model output shape:\", outputs.shape)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # 反向传播与优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/5], Loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), \"icnet_model.pth\")\n",
    "    print(\"Model training complete and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574433/3441475435.py:89: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"icnet_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Average IoU: 0.2514\n",
      "Average Pixel Accuracy: 0.7216\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as F_transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 数据加载器中的标签处理\n",
    "def mask_transform(mask):\n",
    "    mask = mask.resize((128, 128))\n",
    "    mask = F_transforms.pil_to_tensor(mask).squeeze(0)\n",
    "    mask = mask.long()\n",
    "    # 将超出范围的值标记为无效值\n",
    "    mask[(mask < 0) | (mask >= 21)] = -1\n",
    "    return mask\n",
    "\n",
    "# 计算 IoU\n",
    "def compute_iou(predicted, target, num_classes=21):\n",
    "    iou_scores = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = (predicted == cls)\n",
    "        target_mask = (target == cls)\n",
    "\n",
    "        intersection = (pred_mask & target_mask).sum()\n",
    "        union = (pred_mask | target_mask).sum()\n",
    "\n",
    "        if union == 0:\n",
    "            iou_scores.append(float('nan'))  # 忽略该类\n",
    "        else:\n",
    "            iou_scores.append(intersection.item() / union.item())\n",
    "    return np.nanmean(iou_scores)\n",
    "\n",
    "# 计算像素准确率\n",
    "def compute_pixel_accuracy(predicted, target):\n",
    "    valid = (target != -1)  # 忽略无效像素\n",
    "    correct = (predicted[valid] == target[valid]).sum()\n",
    "    total = valid.sum()\n",
    "    return correct.item() / total.item()\n",
    "\n",
    "# 推理并评估\n",
    "def infer_and_evaluate(model, device, data_loader):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    pixel_accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # 推理\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # 计算指标\n",
    "            for pred, target in zip(predicted, masks):\n",
    "                iou = compute_iou(pred.cpu().numpy(), target.cpu().numpy())\n",
    "                pixel_acc = compute_pixel_accuracy(pred.cpu().numpy(), target.cpu().numpy())\n",
    "\n",
    "                iou_scores.append(iou)\n",
    "                pixel_accuracies.append(pixel_acc)\n",
    "\n",
    "    avg_iou = np.nanmean(iou_scores)\n",
    "    avg_pixel_acc = np.mean(pixel_accuracies)\n",
    "    return avg_iou, avg_pixel_acc\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 数据加载与预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = VOCSegmentation(\n",
    "        root='../data',\n",
    "        year='2007',\n",
    "        image_set='val',  # 验证集\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=mask_transform\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # 模型定义\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ICNet(num_classes=21).to(device)\n",
    "    model.load_state_dict(torch.load(\"icnet_model.pth\", map_location=device))\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # 评估模型\n",
    "    avg_iou, avg_pixel_acc = infer_and_evaluate(model, device, data_loader)\n",
    "    print(f\"Average IoU: {avg_iou:.4f}\")\n",
    "    print(f\"Average Pixel Accuracy: {avg_pixel_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.PSPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidPoolingModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pool_sizes):\n",
    "        super(PyramidPoolingModule, self).__init__()\n",
    "        self.stages = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(output_size=max(size, 2)),  # 确保池化结果至少是 2x2\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            for size in pool_sizes\n",
    "        ])\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + len(pool_sizes) * out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]  # 原始输入的空间大小\n",
    "        pooled = [x]  # 添加原始特征\n",
    "        for stage in self.stages:\n",
    "            pooled.append(F.interpolate(stage(x), size=size, mode='bilinear', align_corners=False))\n",
    "        out = torch.cat(pooled, dim=1)\n",
    "        return self.bottleneck(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_path=None):\n",
    "        super(PSPNet, self).__init__()\n",
    "        backbone = resnet50(pretrained=False)\n",
    "\n",
    "        if pretrained_path:\n",
    "            print(f\"Loading pretrained weights from {pretrained_path}\")\n",
    "            state_dict = torch.load(pretrained_path)\n",
    "            backbone.load_state_dict(state_dict)\n",
    "\n",
    "        # ResNet backbone\n",
    "        self.layer0 = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool,\n",
    "        )\n",
    "        self.layer1 = backbone.layer1\n",
    "        self.layer2 = backbone.layer2\n",
    "        self.layer3 = backbone.layer3\n",
    "        self.layer4 = backbone.layer4\n",
    "\n",
    "        # Pyramid Pooling Module\n",
    "        self.ppm = PyramidPoolingModule(in_channels=2048, out_channels=512, pool_sizes=[1, 2, 3, 6])\n",
    "\n",
    "        # Final classification head\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.ppm(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_695053/1537542397.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(pretrained_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Batch 1\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  7, 12, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 2\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 11, 14, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 3\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  8,  9, 12, 13, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 4\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6, 13, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 5\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 10, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 6\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 11, 13, 14, 15, 16, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 7\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 13, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 8\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  8, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 9\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 12, 13, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 10\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  4,  7, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 11\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  8, 12, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 12\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 11, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 13\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  9, 10, 11, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 14\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 11, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 15\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 12, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 16\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 10, 12, 13, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 17\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  9, 11, 16, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 18\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4, 12, 15, 17, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 19\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  9, 10, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 20\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 10, 12, 14, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 21\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  9, 11, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 22\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 12, 14, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 23\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 15, 16, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 24\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6, 13, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 25\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  4,  6, 10, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 26\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  9, 13, 14, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 27\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  6,  7,  9, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 28\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  9, 11, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 29\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0, 10, 12, 15, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 30\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  4, 14, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 31\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7,  8, 10, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 32\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3, 10, 12, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 33\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 34\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  5,  7,  9, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 35\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 36\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  6, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 37\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  8,  9, 15, 16, 18, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 38\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  5,  7,  9, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 39\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  7, 12, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 40\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 11, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 41\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7,  8,  9, 11, 12, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 42\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  5, 11, 14, 15, 16, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 43\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7,  9, 10, 13, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 44\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1, 10, 11, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 45\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  6,  7, 12, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 46\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  9, 15, 17, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 47\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1, 11, 13, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 48\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  5,  8, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 49\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  6,  7, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 50\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 14, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 51\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 12, 16, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 52\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  7,  8, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [1/5] Batch 53\n",
      "Images shape: torch.Size([1, 3, 256, 256])\n",
      "Masks shape: torch.Size([1, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4], device='cuda:0')\n",
      "Input image shape: torch.Size([1, 3, 256, 256])\n",
      "Model output shape: torch.Size([1, 21, 256, 256])\n",
      "Epoch [1/5], Loss: 1.4624\n",
      "Epoch [2/5] Batch 1\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 12, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 2\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  9, 11, 15, 16, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 3\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0, 15, 16, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 4\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  9, 11, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 5\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  9, 11, 13, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 6\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  5, 10, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 7\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  6,  7, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 8\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  2, 13, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 9\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  6,  7,  9, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 10\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  7,  9, 10, 13, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 11\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  7, 10, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 12\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  9, 10, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 13\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2, 12, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 14\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  9, 13, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 15\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  7,  8, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 16\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  7, 14, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 17\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  7,  8, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 18\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6, 11, 12, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 19\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8, 11, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 20\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  8, 11, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 21\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  6, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 22\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  7, 16, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 23\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0, 12, 13, 14, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 24\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 25\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  7,  9, 11, 13, 15, 18, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 26\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 10, 11, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 27\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7,  9, 11, 12, 14, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 28\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  8, 10, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 29\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3, 13, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 30\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 10, 12, 13, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 31\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 12, 15, 17, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 32\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 13, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 33\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 15, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 34\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  7, 14, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 35\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  5,  6, 12, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 36\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 37\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  9, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 38\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6, 12, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 39\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  5, 13, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 40\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4, 16, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 41\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  7, 11, 13, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 42\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 10, 12, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 43\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  9, 13, 15, 16, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 44\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5, 14, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 45\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5, 15, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 46\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6, 12, 14, 15, 16, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 47\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5, 10, 11, 12, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 48\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 12, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 49\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 11, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 50\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8, 10, 12, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 51\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  9, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 52\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [2/5] Batch 53\n",
      "Images shape: torch.Size([1, 3, 256, 256])\n",
      "Masks shape: torch.Size([1, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1], device='cuda:0')\n",
      "Input image shape: torch.Size([1, 3, 256, 256])\n",
      "Model output shape: torch.Size([1, 21, 256, 256])\n",
      "Epoch [2/5], Loss: 1.2668\n",
      "Epoch [3/5] Batch 1\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7,  9, 10, 11, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 2\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  7,  8, 11, 12, 14, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 3\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 11, 13, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 4\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6, 12, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 5\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  7,  9, 12, 15, 16, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 6\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 7\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7,  9, 12, 13, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 8\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  7, 10, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 9\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  8,  9, 15, 16, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 10\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8, 12, 13, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 11\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3, 17, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 12\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  8, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 13\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 13, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 14\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 11, 12, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 15\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2, 10, 11, 12, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 16\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  4, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 17\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  9, 11, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 18\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  6,  7, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 19\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0, 11, 14, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 20\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0, 11, 15, 17, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 21\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 13, 15, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 22\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 14, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 23\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 24\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 12, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 25\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  3,  6,  9, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 26\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  4,  5,  6,  8, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 27\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  5, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 28\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  5, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 29\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7, 13, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 30\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  6,  7, 10, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 31\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  7,  8,  9, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 32\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 10, 11, 13, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 33\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7,  8, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 34\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 12, 15, 16, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 35\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5, 10, 12, 14, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 36\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  6, 13, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 37\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8,  9, 11, 15, 16, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 38\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  8,  9, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 39\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5, 10, 11, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 40\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7, 11, 15, 17, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 41\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7,  9, 15, 16, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 42\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1, 13, 14, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 43\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 10, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 44\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  8, 12, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 45\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8,  9, 10, 13, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 46\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  9, 11, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 47\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  8, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 48\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  7,  9, 12, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 49\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  9, 15, 18, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 50\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0, 12, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 51\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  6, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 52\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  9, 11, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [3/5] Batch 53\n",
      "Images shape: torch.Size([1, 3, 256, 256])\n",
      "Masks shape: torch.Size([1, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3], device='cuda:0')\n",
      "Input image shape: torch.Size([1, 3, 256, 256])\n",
      "Model output shape: torch.Size([1, 21, 256, 256])\n",
      "Epoch [3/5], Loss: 1.2375\n",
      "Epoch [4/5] Batch 1\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2, 10, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 2\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7,  9, 11, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 3\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5, 15, 17, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 4\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  8,  9, 12, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 5\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 6\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  7, 12], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 7\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  6,  9, 14, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 8\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  5,  6,  7,  9, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 9\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8, 15, 17, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 10\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5, 10, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 11\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  9, 11, 16, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 12\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  8, 12, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 13\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8, 12, 14, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 14\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  9, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 15\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  6, 11, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 16\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  7,  9, 12, 13, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 17\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 12, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 18\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 11, 13, 15, 16, 17, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 19\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  8,  9, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 20\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  5,  9, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 21\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  7, 12, 14, 15, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 22\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5, 10, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 23\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3, 12, 14, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 24\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 12, 14, 15, 17, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 25\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 13, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 26\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  5,  8, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 27\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  7, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 28\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 29\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4, 11, 15, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 30\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 14, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 31\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8,  9, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 32\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  8,  9, 11, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 33\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  7, 12, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 34\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 13, 15, 16, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 35\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  9, 13, 15, 16, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 36\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7, 10, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 37\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  9, 11, 15, 16, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 38\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  9, 10, 11, 13, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 39\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  9, 11, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 40\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  8, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 41\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 42\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  7, 10, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 43\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 10, 13, 14, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 44\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  9, 12, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 45\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 46\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  4,  9, 11, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 47\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 11, 12, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 48\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  8, 14, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 49\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  7, 12, 13, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 50\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5, 11, 13, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 51\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  4,  5, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 52\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  6,  7,  9, 11, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [4/5] Batch 53\n",
      "Images shape: torch.Size([1, 3, 256, 256])\n",
      "Masks shape: torch.Size([1, 256, 256])\n",
      "Unique mask values: tensor([-1,  0, 10, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([1, 3, 256, 256])\n",
      "Model output shape: torch.Size([1, 21, 256, 256])\n",
      "Epoch [4/5], Loss: 1.2040\n",
      "Epoch [5/5] Batch 1\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  2,  8, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 2\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8,  9, 10, 11, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 3\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  8,  9, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 4\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 15, 16, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 5\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  6,  9, 10, 13, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 6\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  9, 14, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 7\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3, 10, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 8\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  5, 12, 13, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 9\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 10\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6, 10, 12, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 11\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  7, 10, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 12\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  9, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 13\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  6,  7, 11], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 14\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2, 10, 13, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 15\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  9, 11, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 16\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  5, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 17\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3,  6,  7, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 18\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  6,  7, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 19\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  8, 12, 18, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 20\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  9, 12, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 21\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4, 11, 15, 16, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 22\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 11, 14, 15, 16, 18, 19, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 23\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  5,  6,  7,  9, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 24\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4,  5,  7, 11, 14, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 25\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  8, 14, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 26\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  3, 11, 13, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 27\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8, 12, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 28\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6,  7,  9, 15, 16, 18, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 29\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  7,  8,  9, 16, 18, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 30\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  4,  5,  9, 11, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 31\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  9, 14, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 32\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0, 11, 15, 16, 17, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 33\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  7,  9, 11, 14, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 34\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8, 12, 13, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 35\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  8, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 36\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  7, 10, 12, 15, 16], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 37\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 10, 11, 12, 15, 18, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 38\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5, 12, 15, 17, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 39\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 12, 14, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 40\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  4, 10], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 41\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  6, 13, 15, 19], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 42\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  5,  7,  9, 15, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 43\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  3,  5,  8, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 44\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  8,  9, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 45\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  7,  8, 12, 15, 17, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 46\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  9, 12, 13, 15, 16, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 47\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  8, 11, 12, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 48\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  5,  6,  9, 11, 15, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 49\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  1,  7,  9, 11, 12, 15], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 50\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  2,  8,  9, 15, 16, 20], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 51\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  9, 15, 16, 18], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 52\n",
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Masks shape: torch.Size([4, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4,  6, 13, 15, 17], device='cuda:0')\n",
      "Input image shape: torch.Size([4, 3, 256, 256])\n",
      "Model output shape: torch.Size([4, 21, 256, 256])\n",
      "Epoch [5/5] Batch 53\n",
      "Images shape: torch.Size([1, 3, 256, 256])\n",
      "Masks shape: torch.Size([1, 256, 256])\n",
      "Unique mask values: tensor([-1,  0,  4], device='cuda:0')\n",
      "Input image shape: torch.Size([1, 3, 256, 256])\n",
      "Model output shape: torch.Size([1, 21, 256, 256])\n",
      "Epoch [5/5], Loss: 1.2319\n",
      "Model training complete and saved to pspnet_model.pth.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as F_transforms\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn.functional as F\n",
    "# 自定义标签处理函数\n",
    "def mask_transform(mask):\n",
    "    mask = mask.resize((256, 256))  # 调整标签大小与输入一致\n",
    "    mask = F_transforms.pil_to_tensor(mask).squeeze(0)\n",
    "    mask = mask.long()\n",
    "\n",
    "    # 将超出范围的值标记为无效值\n",
    "    mask[(mask < 0) | (mask >= 21)] = -1\n",
    "    return mask\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 数据加载与预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = VOCSegmentation(\n",
    "        root='../data',\n",
    "        year='2007',\n",
    "        image_set='train',\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=mask_transform\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    # 手动下载的预训练模型路径\n",
    "    pretrained_path = \"resnet50-0676ba61.pth\"\n",
    "\n",
    "    # 模型定义\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PSPNet(num_classes=21, pretrained_path=pretrained_path).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 训练循环\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):  # 假设训练 5 个 epoch\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (images, masks) in enumerate(data_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # 打印调试信息\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] Batch {batch_idx + 1}\")\n",
    "            print(\"Images shape:\", images.shape)\n",
    "            print(\"Masks shape:\", masks.shape)\n",
    "            print(\"Unique mask values:\", torch.unique(masks))\n",
    "\n",
    "            # 前向传播\n",
    "            print(\"Input image shape:\", images.shape)\n",
    "            outputs = model(images)\n",
    "            print(\"Model output shape:\", outputs.shape)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # 反向传播与优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader):.4f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_save_path = \"pspnet_model.pth\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model training complete and saved to {model_save_path}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_695053/707264380.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 0.0695\n",
      "Mean Pixel Accuracy: 0.2025\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F_transforms\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 自定义标签处理函数\n",
    "def mask_transform(mask):\n",
    "    mask = mask.resize((256, 256))  # 调整标签大小与输入一致\n",
    "    mask = F_transforms.pil_to_tensor(mask).squeeze(0)\n",
    "    mask = mask.long()\n",
    "    mask[(mask < 0) | (mask >= 21)] = -1  # 将超出范围的值标记为无效值\n",
    "    return mask\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "def load_model(model_path, num_classes=21, device=\"cuda\"):\n",
    "    model = PSPNet(num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  # 设置为评估模式\n",
    "    return model\n",
    "\n",
    "\n",
    "# 计算 IoU\n",
    "def calculate_iou(preds, targets, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_mask = preds == cls\n",
    "        target_mask = targets == cls\n",
    "        intersection = (pred_mask & target_mask).sum()\n",
    "        union = (pred_mask | target_mask).sum()\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # 忽略该类别\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    return np.nanmean(ious)  # 计算所有类的平均 IoU\n",
    "\n",
    "\n",
    "# 计算像素精度\n",
    "def calculate_pixel_accuracy(preds, targets):\n",
    "    valid = targets != -1  # 忽略无效值\n",
    "    correct = (preds == targets) & valid\n",
    "    accuracy = correct.sum() / valid.sum()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# 推理函数\n",
    "def evaluate_model(model, data_loader, device=\"cuda\", num_classes=21):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    pixel_accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(data_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # 模型前向推理\n",
    "            outputs = model(images)  # [batch_size, num_classes, H, W]\n",
    "            preds = torch.argmax(outputs, dim=1)  # [batch_size, H, W]\n",
    "\n",
    "            # 计算指标\n",
    "            for pred, mask in zip(preds, masks):\n",
    "                iou = calculate_iou(pred.cpu().numpy(), mask.cpu().numpy(), num_classes)\n",
    "                pixel_acc = calculate_pixel_accuracy(pred.cpu().numpy(), mask.cpu().numpy())\n",
    "                iou_scores.append(iou)\n",
    "                pixel_accuracies.append(pixel_acc)\n",
    "\n",
    "    mean_iou = np.nanmean(iou_scores)  # 忽略 NaN\n",
    "    mean_pixel_accuracy = np.mean(pixel_accuracies)\n",
    "    return mean_iou, mean_pixel_accuracy\n",
    "\n",
    "\n",
    "# 主推理逻辑\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_path = \"pspnet_model.pth\"\n",
    "\n",
    "    # 数据加载与预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = VOCSegmentation(\n",
    "        root='../data',\n",
    "        year='2007',\n",
    "        image_set='val',\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=mask_transform\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # 加载模型\n",
    "    model = load_model(model_path, num_classes=21, device=device)\n",
    "\n",
    "    # 数据集评估\n",
    "    mean_iou, mean_pixel_accuracy = evaluate_model(model, data_loader, device=device, num_classes=21)\n",
    "\n",
    "    # 打印性能指标\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"Mean Pixel Accuracy: {mean_pixel_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.DeepLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ResNet50 backbone weights from resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_700580/3166456681.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeepLab pretrained weights from deeplabv3_resnet50_coco-cd0a2569.pth\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_700580/3166456681.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(deeplab_pretrained_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Epoch [1/5], Loss: 1.5484\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Epoch [2/5], Loss: 1.2783\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Epoch [3/5], Loss: 1.2416\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Epoch [4/5], Loss: 1.2089\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Backbone Output Shape: torch.Size([4, 2048, 32, 32])\n",
      "Classifier Output Shape: torch.Size([4, 21, 32, 32])\n",
      "Epoch [5/5], Loss: 1.1612\n",
      "Model training complete and saved to deeplab_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision.transforms import functional as F_transforms\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# 自定义标签处理函数\n",
    "def mask_transform(mask):\n",
    "    mask = mask.resize((256, 256))  # 调整标签大小\n",
    "    mask = F_transforms.pil_to_tensor(mask).squeeze(0)\n",
    "    mask = mask.long()\n",
    "    mask[(mask < 0) | (mask >= 21)] = -1  # 将超出范围的值标记为无效值\n",
    "    return mask\n",
    "\n",
    "\n",
    "# 自定义 ResNet50 加载函数\n",
    "def load_custom_resnet50(weights_path=None, device=\"cuda\"):\n",
    "    # 构建 ResNet50 Backbone\n",
    "    model = ResNet(\n",
    "        block=Bottleneck,\n",
    "        layers=[3, 4, 6, 3],  # ResNet-50 配置\n",
    "        replace_stride_with_dilation=[False, True, True]\n",
    "    )\n",
    "    if weights_path:\n",
    "        print(f\"Loading ResNet50 backbone weights from {weights_path}\")\n",
    "        state_dict = torch.load(weights_path, map_location=device)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # 删除最后的全连接层及分类相关层，保留到最后的卷积层\n",
    "    model = nn.Sequential(*list(model.children())[:-2])\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# 自定义 DeepLabV3 模型加载函数\n",
    "def load_deeplab_model(num_classes=21, deeplab_pretrained_path=None, resnet_pretrained_path=None, device=\"cuda\"):\n",
    "    # 加载自定义的 ResNet50 Backbone\n",
    "    backbone = load_custom_resnet50(weights_path=resnet_pretrained_path, device=device)\n",
    "\n",
    "    # 构建 DeepLab 模型\n",
    "    class DeepLabV3(nn.Module):\n",
    "        def __init__(self, backbone, num_classes):\n",
    "            super(DeepLabV3, self).__init__()\n",
    "            self.backbone = backbone\n",
    "            self.classifier = DeepLabHead(2048, num_classes)  # 2048是 ResNet 的输出通道数\n",
    "\n",
    "        def forward(self, x):\n",
    "            features = self.backbone(x)  # 获取特征图\n",
    "            return {\"out\": self.classifier(features)}\n",
    "\n",
    "    model = DeepLabV3(backbone, num_classes)\n",
    "\n",
    "    # 加载 DeepLab 的预训练权重\n",
    "    if deeplab_pretrained_path:\n",
    "        print(f\"Loading DeepLab pretrained weights from {deeplab_pretrained_path}\")\n",
    "        state_dict = torch.load(deeplab_pretrained_path, map_location=device)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device=\"cuda\"):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # 前向传播调试\n",
    "            features = model.backbone(images)\n",
    "            print(\"Backbone Output Shape:\", features.shape)\n",
    "\n",
    "            outputs = model.classifier(features)\n",
    "            print(\"Classifier Output Shape:\", outputs.shape)\n",
    "            # 前向传播\n",
    "            outputs = model(images)['out']  # DeepLab 返回一个字典\n",
    "            outputs = nn.functional.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)  # 上采样\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # 反向传播与优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# 主训练逻辑\n",
    "def main():\n",
    "    # 配置参数\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = 21\n",
    "    num_epochs = 5\n",
    "    batch_size = 4\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # 数据预处理与加载\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = VOCSegmentation(\n",
    "        root='../data',\n",
    "        year='2007',\n",
    "        image_set='train',\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=mask_transform\n",
    "    )\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # 手动下载的预训练权重路径\n",
    "    resnet_pretrained_path = \"resnet50-0676ba61.pth\"\n",
    "    deeplab_pretrained_path = \"deeplabv3_resnet50_coco-cd0a2569.pth\"\n",
    "\n",
    "    # 加载模型\n",
    "    model = load_deeplab_model(\n",
    "        num_classes=num_classes,\n",
    "        deeplab_pretrained_path=deeplab_pretrained_path,\n",
    "        resnet_pretrained_path=resnet_pretrained_path,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # 忽略无效标签\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs, device=device)\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), \"deeplab_model.pth\")\n",
    "    print(\"Model training complete and saved to deeplab_model.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ResNet50 backbone weights from resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_700580/2830683180.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resnet_pretrained_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeepLab pretrained weights from deeplab_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_700580/2830683180.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(deeplab_pretrained_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 0.0374\n",
      "Pixel Accuracy: 0.7022\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import functional as F_transforms\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 自定义标签处理函数\n",
    "def mask_transform(mask):\n",
    "    mask = mask.resize((256, 256))  # 调整标签大小\n",
    "    mask = F_transforms.pil_to_tensor(mask).squeeze(0)\n",
    "    mask = mask.long()\n",
    "    mask[(mask < 0) | (mask >= 21)] = -1  # 将超出范围的值标记为无效值\n",
    "    return mask\n",
    "\n",
    "\n",
    "# 加载模型函数\n",
    "def load_deeplab_model(num_classes=21, deeplab_pretrained_path=None, resnet_pretrained_path=None, device=\"cuda\"):\n",
    "    # 构建 ResNet50 Backbone\n",
    "    model = ResNet(\n",
    "        block=Bottleneck,\n",
    "        layers=[3, 4, 6, 3],  # ResNet-50 配置\n",
    "        replace_stride_with_dilation=[False, True, True]\n",
    "    )\n",
    "    if resnet_pretrained_path:\n",
    "        print(f\"Loading ResNet50 backbone weights from {resnet_pretrained_path}\")\n",
    "        state_dict = torch.load(resnet_pretrained_path, map_location=device)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # 删除最后的全连接层及分类相关层，保留到最后的卷积层\n",
    "    backbone = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "    # 构建 DeepLab 模型\n",
    "    class DeepLabV3(nn.Module):\n",
    "        def __init__(self, backbone, num_classes):\n",
    "            super(DeepLabV3, self).__init__()\n",
    "            self.backbone = backbone\n",
    "            self.classifier = DeepLabHead(2048, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            features = self.backbone(x)\n",
    "            return {\"out\": self.classifier(features)}\n",
    "\n",
    "    model = DeepLabV3(backbone, num_classes)\n",
    "\n",
    "    # 加载 DeepLab 的预训练权重\n",
    "    if deeplab_pretrained_path:\n",
    "        print(f\"Loading DeepLab pretrained weights from {deeplab_pretrained_path}\")\n",
    "        state_dict = torch.load(deeplab_pretrained_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# 推理函数并计算指标\n",
    "def evaluate(model, data_loader, device=\"cuda\"):\n",
    "    intersection = torch.zeros(21).to(device)  # 交集\n",
    "    union = torch.zeros(21).to(device)         # 并集\n",
    "    total_correct = 0                          # 总正确像素\n",
    "    total_pixels = 0                           # 总像素\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # 模型推理\n",
    "            outputs = model(images)['out']\n",
    "            outputs = nn.functional.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)  # 调整尺寸\n",
    "            predictions = torch.argmax(outputs, dim=1)  # 获取预测类别\n",
    "\n",
    "            # 计算指标\n",
    "            for cls in range(21):\n",
    "                pred_cls = predictions == cls\n",
    "                true_cls = masks == cls\n",
    "                intersection[cls] += torch.sum(pred_cls & true_cls)\n",
    "                union[cls] += torch.sum(pred_cls | true_cls)\n",
    "\n",
    "            total_correct += torch.sum(predictions == masks).item()\n",
    "            total_pixels += masks.numel()\n",
    "\n",
    "    # 计算 mIoU 和像素准确率\n",
    "    iou = intersection / (union + 1e-6)\n",
    "    miou = torch.mean(iou).item()\n",
    "    pixel_accuracy = total_correct / total_pixels\n",
    "    return miou, pixel_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# 主推理逻辑\n",
    "def main_inference():\n",
    "    # 配置参数\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = 21\n",
    "    batch_size = 4\n",
    "    resnet_pretrained_path = \"resnet50-0676ba61.pth\"\n",
    "    deeplab_pretrained_path = \"deeplab_model.pth\"  # 已训练的模型权重\n",
    "\n",
    "    # 数据预处理与加载\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = VOCSegmentation(\n",
    "        root='../data',\n",
    "        year='2007',\n",
    "        image_set='val',  # 验证集\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=mask_transform\n",
    "    )\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 加载模型\n",
    "    model = load_deeplab_model(\n",
    "        num_classes=num_classes,\n",
    "        deeplab_pretrained_path=deeplab_pretrained_path,\n",
    "        resnet_pretrained_path=resnet_pretrained_path,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 评估\n",
    "    miou, pixel_accuracy = evaluate(model, data_loader, device=device)\n",
    "    print(f\"Mean IoU: {miou:.4f}\")\n",
    "    print(f\"Pixel Accuracy: {pixel_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_inference()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.Pixel-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 938/938 [00:13<00:00, 68.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 938/938 [00:13<00:00, 70.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 938/938 [00:13<00:00, 70.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 938/938 [00:13<00:00, 69.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 938/938 [00:13<00:00, 68.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.0010\n",
      "Model training complete and saved to pixel_rnn_mnist.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义 PixelRNN 模型\n",
    "class PixelRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(PixelRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(num_classes, input_dim)\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 嵌入输入（独热编码 -> 嵌入）\n",
    "        x = self.embedding(x.long())  # 输入为整数类型\n",
    "        x = x.view(x.size(0), -1, x.size(-1))  # 展平图像\n",
    "        # 经过 RNN\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        # 全连接输出\n",
    "        out = self.fc(out)\n",
    "        return out.view(-1, 28, 28, 256)  # 输出与输入图像大小一致\n",
    "\n",
    "# 数据加载与预处理\n",
    "def load_data(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # 转换为张量\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='../data', train=True, transform=transform, download=False)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = images.to(device)\n",
    "            # 将输入值映射到 [0, 255] 的整数值范围\n",
    "            targets = (images * 255).clamp(0, 255).long()\n",
    "            targets = targets.squeeze(1)  # 去掉单通道维度\n",
    "            \n",
    "            # 检查目标值范围\n",
    "            assert targets.min() >= 0 and targets.max() <= 255, \"Target values out of range!\"\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(targets)\n",
    "            outputs = outputs.permute(0, 3, 1, 2)  # 调整维度为 (batch_size, num_classes, height, width)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 64\n",
    "    num_epochs = 5\n",
    "    learning_rate = 0.001\n",
    "    input_dim = 16  # 嵌入维度\n",
    "    hidden_dim = 64  # RNN 隐藏层维度\n",
    "    num_classes = 256  # 灰度值范围\n",
    "\n",
    "    # 加载数据\n",
    "    train_loader = load_data(batch_size)\n",
    "\n",
    "    # 初始化模型、损失函数、优化器\n",
    "    model = PixelRNN(input_dim, hidden_dim, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    train_model(model, train_loader, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), \"pixel_rnn_mnist.pth\")\n",
    "    print(\"Model training complete and saved to pixel_rnn_mnist.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713101/1720925710.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"pixel_rnn_mnist.pth\", map_location=device))\n",
      "Inference: 100%|██████████| 157/157 [00:01<00:00, 145.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Pixel Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义计算像素准确率的函数\n",
    "def calculate_pixel_accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    计算像素准确率（Pixel Accuracy）。\n",
    "    :param predictions: 模型输出的预测值 (B, H, W)\n",
    "    :param targets: 实际目标值 (B, H, W)\n",
    "    :return: 像素准确率\n",
    "    \"\"\"\n",
    "    correct = (predictions == targets).sum().item()\n",
    "    total = targets.numel()  # 总像素数\n",
    "    return correct / total\n",
    "\n",
    "# 定义推理函数\n",
    "def infer_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_pixel_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(test_loader, desc=\"Inference\"):\n",
    "            images = images.to(device)\n",
    "            targets = (images * 255).clamp(0, 255).long()\n",
    "            targets = targets.squeeze(1)  # 去掉单通道维度\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(targets)\n",
    "            outputs = outputs.permute(0, 3, 1, 2)  # 调整维度为 (batch_size, num_classes, height, width)\n",
    "\n",
    "            # 获取每个像素的预测类别\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # 计算像素准确率\n",
    "            pixel_accuracy = calculate_pixel_accuracy(predictions, targets)\n",
    "            total_pixel_accuracy += pixel_accuracy\n",
    "\n",
    "    # 返回平均像素准确率\n",
    "    avg_pixel_accuracy = total_pixel_accuracy / len(test_loader)\n",
    "    return avg_pixel_accuracy\n",
    "\n",
    "# 主推理逻辑\n",
    "def main_inference():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 64\n",
    "    input_dim = 16  # 嵌入维度\n",
    "    hidden_dim = 64  # RNN 隐藏层维度\n",
    "    num_classes = 256  # 灰度值范围\n",
    "\n",
    "    # 数据加载\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # 转换为张量\n",
    "    ])\n",
    "    test_dataset = datasets.MNIST(root='../data', train=False, transform=transform, download=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 加载模型\n",
    "    model = PixelRNN(input_dim, hidden_dim, num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(\"pixel_rnn_mnist.pth\", map_location=device))\n",
    "\n",
    "    # 推理并计算指标\n",
    "    avg_pixel_accuracy = infer_model(model, test_loader, device)\n",
    "    print(f\"Average Pixel Accuracy: {avg_pixel_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_inference()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3439513114.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -m pip install --upgrade pip\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_721552/3135048644.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 自定义数据集类\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, vocab, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review, sentiment = self.data[idx]\n",
    "        tokens = self.tokenizer(review)[:self.max_length]  # 分词并截断\n",
    "        indices = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]  # 转为索引\n",
    "        indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "        label = 0 if sentiment == \"negative\" else 1  # 负面=0，正面=1\n",
    "        return indices_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# 加载 CSV 数据\n",
    "def load_imdb_from_csv(csv_path, test_size=0.2, max_length=256, batch_size=32):\n",
    "    # 加载数据\n",
    "    df = pd.read_csv(csv_path)\n",
    "    reviews = df[\"review\"].tolist()\n",
    "    sentiments = df[\"sentiment\"].tolist()\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    train_data, test_data = train_test_split(\n",
    "        list(zip(reviews, sentiments)), test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # 定义分词器\n",
    "    tokenizer = lambda x: x.lower().split()\n",
    "\n",
    "    # 构建词汇表\n",
    "    vocab = {\"<unk>\": 0}  # 未知词映射到索引 0\n",
    "    for review, _ in train_data:\n",
    "        for token in tokenizer(review):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "    # 构建 Dataset\n",
    "    train_dataset = IMDBDataset(train_data, tokenizer, vocab, max_length)\n",
    "    test_dataset = IMDBDataset(test_data, tokenizer, vocab, max_length)\n",
    "\n",
    "    # 构建 DataLoader\n",
    "    def collate_batch(batch):\n",
    "        texts = [item[0] for item in batch]\n",
    "        labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "        texts = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<unk>\"])  # 填充\n",
    "        return texts, labels\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    return train_loader, test_loader, vocab\n",
    "\n",
    "# 测试数据加载\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"./data/IMDB/IMDB_Dataset.csv\"  # 确保文件路径正确\n",
    "    train_loader, test_loader, vocab = load_imdb_from_csv(csv_path, batch_size=32)\n",
    "\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "    for texts, labels in train_loader:\n",
    "        print(\"Texts shape:\", texts.shape)\n",
    "        print(\"Labels shape:\", labels.shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fushaomin/miniconda3/lib/python3.12/site-packages/torchtext/__init__.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\\n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \\n\"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/fushaomin/miniconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMDB\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, BucketIterator\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_ops.py:1350\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1345\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1350\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/ctypes/__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /home/fushaomin/miniconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. 数据预处理\n",
    "TEXT = Field(sequential=True, tokenize='spacy', tokenizer_language='en_core_web_sm', include_lengths=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False, is_target=True)\n",
    "\n",
    "# 加载IMDB数据集\n",
    "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# 构建词汇表\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors='glove.6B.100d', unk_init=torch.Tensor.normal_)\n",
    "\n",
    "# 创建数据迭代器\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 2. 定义LSTM模型\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # 双向LSTM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 3. 初始化模型\n",
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 1  # 二分类\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = SentimentLSTM(input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout).to(device)\n",
    "\n",
    "# 使用GloVe词向量初始化embedding层\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "# 4. 训练模型\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        labels = batch.label.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = accuracy_score(labels.cpu().numpy(), predictions.cpu().round().detach().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# 5. 训练过程\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "# 6. 保存模型\n",
    "torch.save(model.state_dict(), 'sentiment_lstm.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.GNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f0020b13a10>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /packages/98/b8/12abefe9d8830797dcea4c822e503eede1128e44ef0fef6fdd80a8a1eb47/spacy-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f0020b13e00>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /packages/98/b8/12abefe9d8830797dcea4c822e503eede1128e44ef0fef6fdd80a8a1eb47/spacy-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading spacy-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.11-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.13.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.1-py3-none-any.whl.metadata (169 kB)\n",
      "Requirement already satisfied: jinja2 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/fushaomin/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m193.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.11-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading pydantic-2.10.1-py3-none-any.whl (455 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m442.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "Downloading thinc-8.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m326.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m553.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.13.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m365.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m310.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m657.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, click, catalogue, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed annotated-types-0.7.0 blis-1.0.1 catalogue-2.0.10 click-8.1.7 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.11 numpy-2.0.2 preshed-3.0.9 pydantic-2.10.1 pydantic-core-2.27.1 rich-13.9.4 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 typer-0.13.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f4806ed23c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /packages/0b/f0/89ee2bc9da434bd78464f288fdb346bc2932f2ee80a90b2a4bbbac262c74/sacremoses-0.1.1-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting regex (from sacremoses)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/http/client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/ssl.py\", line 1251, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/ssl.py\", line 1103, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/commands/install.py\", line 379, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_updated_criteria(candidate)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 230, in _get_updated_criteria\n",
      "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
      "    return bool(self._sequence)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 174, in __bool__\n",
      "    return any(self)\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 162, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 53, in _iter_built\n",
      "    candidate = func()\n",
      "                ^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 186, in _make_candidate_from_link\n",
      "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 232, in _make_base_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "                                       ^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 303, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 158, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 235, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 314, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 521, in prepare_linked_requirement\n",
      "    metadata_dist = self._fetch_metadata_only(req)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 373, in _fetch_metadata_only\n",
      "    return self._fetch_metadata_using_link_data_attr(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 393, in _fetch_metadata_using_link_data_attr\n",
      "    metadata_file = get_http_url(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 111, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/network/download.py\", line 148, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_695053/3232696008.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/home/fushaomin/miniconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sacremoses import MosesTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 数据配置\n",
    "DATA_DIR = \"data/wmt14\"  # 替换为你的数据路径\n",
    "TRAIN_SRC_FILE = f\"{DATA_DIR}/train.en\"\n",
    "TRAIN_TGT_FILE = f\"{DATA_DIR}/train.de\"\n",
    "VALIDATION_SRC_FILE = f\"{DATA_DIR}/val.en\"\n",
    "VALIDATION_TGT_FILE = f\"{DATA_DIR}/val.de\"\n",
    "\n",
    "# 加载数据文件\n",
    "def load_data(src_file, tgt_file):\n",
    "    with open(src_file, 'r', encoding='utf-8') as src_f, open(tgt_file, 'r', encoding='utf-8') as tgt_f:\n",
    "        src_sentences = src_f.readlines()\n",
    "        tgt_sentences = tgt_f.readlines()\n",
    "    return src_sentences, tgt_sentences\n",
    "\n",
    "train_src, train_tgt = load_data(TRAIN_SRC_FILE, TRAIN_TGT_FILE)\n",
    "val_src, val_tgt = load_data(VALIDATION_SRC_FILE, VALIDATION_TGT_FILE)\n",
    "\n",
    "# 分词器\n",
    "tokenizer = MosesTokenizer(lang=\"en\")\n",
    "\n",
    "# 构建词汇表\n",
    "def build_vocab(sentences, tokenizer, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]):\n",
    "    vocab = {special: idx for idx, special in enumerate(specials)}\n",
    "    idx = len(specials)\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = idx\n",
    "                idx += 1\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_vocab(train_src, tokenizer.tokenize)\n",
    "tgt_vocab = build_vocab(train_tgt, tokenizer.tokenize)\n",
    "\n",
    "# 将句子转换为张量\n",
    "def sentence_to_tensor(sentence, vocab, tokenizer):\n",
    "    tokens = tokenizer(sentence)\n",
    "    return torch.tensor([vocab[\"<bos>\"]] + [vocab.get(token, vocab[\"<unk>\"]) for token in tokens] + [vocab[\"<eos>\"]])\n",
    "\n",
    "# 自定义数据集\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, tokenizer):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tensor = sentence_to_tensor(self.src_sentences[idx], self.src_vocab, self.tokenizer)\n",
    "        tgt_tensor = sentence_to_tensor(self.tgt_sentences[idx], self.tgt_vocab, self.tokenizer)\n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "# 数据加载器\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab[\"<pad>\"])\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab[\"<pad>\"])\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_tgt, src_vocab, tgt_vocab, tokenizer.tokenize)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# GNMT 模型定义\n",
    "class GNMT(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, hidden_size, num_layers, dropout=0.1):\n",
    "        super(GNMT, self).__init__()\n",
    "        self.encoder = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.decoder = nn.Embedding(tgt_vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embed = self.encoder(src)\n",
    "        tgt_embed = self.decoder(tgt)\n",
    "        outputs, _ = self.lstm(torch.cat([src_embed, tgt_embed], dim=1))\n",
    "        logits = self.fc(outputs)\n",
    "        return logits\n",
    "\n",
    "# 训练配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "model = GNMT(len(src_vocab), len(tgt_vocab), embed_size, hidden_size, num_layers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[\"<pad>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for src, tgt in train_dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src[:, :-1], tgt[:, :-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "train_model(model, train_dataloader, criterion, optimizer)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"gnmt_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sacremoses import MosesTokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# GNMT 模型定义（与训练时保持一致）\n",
    "class GNMT(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, hidden_size, num_layers, dropout=0.1):\n",
    "        super(GNMT, self).__init__()\n",
    "        self.encoder = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.decoder = nn.Embedding(tgt_vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embed = self.encoder(src)\n",
    "        tgt_embed = self.decoder(tgt)\n",
    "        outputs, _ = self.lstm(torch.cat([src_embed, tgt_embed], dim=1))\n",
    "        logits = self.fc(outputs)\n",
    "        return logits\n",
    "\n",
    "# 加载词汇表\n",
    "src_vocab = torch.load(\"src_vocab.pth\")  # 替换为训练时保存的 src_vocab\n",
    "tgt_vocab = torch.load(\"tgt_vocab.pth\")  # 替换为训练时保存的 tgt_vocab\n",
    "\n",
    "# 加载模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "model = GNMT(len(src_vocab), len(tgt_vocab), embed_size, hidden_size, num_layers, dropout).to(device)\n",
    "model.load_state_dict(torch.load(\"gnmt_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 翻译函数\n",
    "def translate(model, sentence, src_vocab, tgt_vocab, tokenizer, max_len=50):\n",
    "    src_tensor = torch.tensor([src_vocab[\"<bos>\"]] + [src_vocab[token] for token in tokenizer(sentence)] + [src_vocab[\"<eos>\"]]).unsqueeze(0).to(device)\n",
    "    tgt_tensor = torch.tensor([tgt_vocab[\"<bos>\"]]).unsqueeze(0).to(device)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, tgt_tensor)\n",
    "        next_token = output.argmax(2)[:, -1]\n",
    "        tgt_tensor = torch.cat([tgt_tensor, next_token.unsqueeze(0)], dim=1)\n",
    "        if next_token.item() == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    return \" \".join([tgt_vocab.itos[idx] for idx in tgt_tensor.squeeze(0).tolist()[1:-1]])\n",
    "\n",
    "# 示例推理\n",
    "tokenizer = MosesTokenizer(lang=\"en\")\n",
    "sentence = \"This is a test sentence.\"\n",
    "translation = translate(model, sentence, src_vocab, tgt_vocab, tokenizer)\n",
    "print(\"Translated Sentence:\", translation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
