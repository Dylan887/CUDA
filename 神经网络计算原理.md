# 神经网络构建原理(以MINIST为例)

在 MNIST 手写数字识别任务中，构建神经网络并训练模型来进行分类是经典的深度学习应用。MNIST 数据集包含 28x28 像素的手写数字图像（0-9），任务是构建一个神经网络，能够根据输入的图像预测对应的数字。本文将通过该案例详细介绍神经网络的逻辑框架和具体的计算流程。
## 神经网络构建框架

**1.数据预处理**
* 将输入数据进行标准化处理（归一化），并将标签转换为适合模型的格式（ one-hot 编码）。

**2.模型构建**
* **a.输入层**：定义输入的大小（将 28x28 的图像展平为 784 维向量）。
* **b.隐藏层**：添加一个或多个隐藏层，每层包含一定数量的神经元，并应用激活函数（如 ReLU）[实际上神经元相当于维数，该过程即是对原特征维数进行扩维或降维]。
* **c.输出层**：定义输出层的神经元数量（与分类类别一致），通常使用 Softmax 函数将输出转换为概率。

**3.前向传播**

* 执行输入层到隐藏层、再到输出层的矩阵乘法和激活函数，计算输出值。

**4.损失函数计算**

* 使用交叉熵等损失函数，计算预测输出与真实标签之间的误差。

**5.反向传播**

* 通过链式法则，计算损失函数对每个参数的偏导数，并更新权重和偏置项。

**6.优化器更新**

* 使用优化器（如 SGD、Adam）基于计算的梯度更新模型参数，降低损失值。

**7.迭代训练**

* 不断重复前向传播、损失计算和反向传播，直到损失收敛或达到设定的训练轮次。

**8.模型评估与预测**

* 训练完成后，用测试数据评估模型性能，并进行新数据的预测。

![alt text](/images/2024-09-20_164314.jpg)

![alt text](/images/2024-09-20_193110.jpg)


## 神经网络的具体计算流程



接下来以MINIST手写数字识别为例，模拟神经网络构建的具体计算过程。

假设该网络包含 **两个隐藏层**，每个隐藏层有 **25 个神经元**，最后的输出层为 **10 个神经元**。

### 1. 前向传播（Forward Propagation）

#### **1.1输入层到第一个隐藏层：**

* **输入大小**：假设输入图像是 28x28 的像素矩阵，展平成 784 维的向量。$$x \in \mathbb{R}^{784 \times 1}$$

* **权重矩阵**：连接输入层到第一个隐藏层的权重矩阵，大小为 25x784，[因为特征向量是列向量，所以需要转置]。$$W_1 \in \mathbb{R}^{25 \times 784}$$

* **偏置项**：第一个隐藏层的偏置项，大小为 25x1。$$b_1 \in \mathbb{R}^{25 \times 1}$$
* **激活函数**：使用 ReLU 激活函数。

**计算步骤**：

* 执行矩阵乘法$$z_1 = W_1 \cdot x + b_1$$



$z_1$ 的维度是 25x1。

* 应用 ReLU 激活函数：$$h_1 = \text{ReLU}(z_1)$$



其中：$$\text{ReLU}(z_1) = \max(0, z_1)$$



* **结果**：第一个隐藏层的输出 $h_1$ 是 25 维的向量。
![alt text](/images/2024-09-20_193035.jpg)

 #### **1.2第一个隐藏层到第二个隐藏层：**

* **权重矩阵**：连接第一个隐藏层到第二个隐藏层的权重矩阵，大小为 25x25。$$W_2 \in \mathbb{R}^{25 \times 25}$$



* **偏置项**：第二个隐藏层的偏置项，大小为 25x1。$$b_2 \in \mathbb{R}^{25 \times 1}$$



**计算步骤**：

* 执行矩阵乘法$$z_2 = W_2 \cdot h_1 + b_2$$



$z_2$的维度是 25x1。

* 应用 ReLU 激活函数：$$h_2 = \text{ReLU}(z_2)$$		



* **结果**：第二个隐藏层的输出 $h_2$ 仍然是 25 维的向量。

#### **1.3第二个隐藏层到输出层：**

* **权重矩阵**：连接第二个隐藏层到输出层的权重矩阵，大小为 10x25。$$W_3 \in \mathbb{R}^{10 \times 25}$$



* **偏置项**：输出层的偏置项，大小为 10x1。$b_3 \in \mathbb{R}^{10 \times 1}$



**计算步骤**：

* 执行矩阵乘法$$z_3 = W_3 \cdot h_2 + b_3$$



$z_3$的维度是 10x1。

* 应用 Softmax 函数将输出转换为概率：$$\text{Softmax}(z_3)_i = \frac{e^{z_{3i}}}{\sum_{j=1}^{10} e^{z_{3j}}}$$



Softmax 输出是 10 维的概率向量，表示输入属于 0-9 的概率。

### 2. 损失计算

使用交叉熵损失函数来计算预测输出与真实标签之间的误差,假设真实标签是 one-hot 编码的向量$y \in \mathbb{R}^{10}$,其中,
$y_i = 1$ 表示真实类别，$p_i = \text{Softmax}(z_3)_i$ 表示模型对类别 $i$ 的预测概率。
![alt text](/images/2024-09-20_192500.jpg)

**交叉熵损失公式**：$$L = -\sum_{i=1}^{10} y_i \log(p_i)$$



**损失计算步骤**：

* 对于每一个样本，计算预测类别对应的概率 $p_i$ 的对数，然后计算损失 $L$。

### 3. 反向传播（Backward Propagation）

反向传播的目标是通过链式法则计算损失函数对每层权重的偏导数，并更新权重矩阵。

#### 3.1输出层到第二个隐藏层：

* **计算损失对输出层的导数**：$$\frac{\partial L}{\partial z_3} = \text{Softmax}(z_3) - y$$



* **计算损失对$W_3$的导数**：$$\frac{\partial L}{\partial W_3} = \frac{\partial L}{\partial z_3} \cdot h_2^T$$



* **计算损失对$b_3$的导数**：$$\frac{\partial L}{\partial b_3} = \frac{\partial L}{\partial z_3}$$



#### 3.2第二个隐藏层到第一个隐藏层：

* **损失传播到第二层的输出$h_2$**：$$\frac{\partial L}{\partial h_2} = W_3^T \cdot \frac{\partial L}{\partial z_3}$$



* **计算 ReLU 激活函数的导数**：$$\frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial h_2} \cdot \text{ReLU}'(z_2)$$



其中：$$\text{ReLU}'(z_2) = 
\begin{cases} 
1 & \text{if } z_2 > 0 \\
0 & \text{if } z_2 \leq 0 
\end{cases}$$



* **计算损失对$W_2$的导数**：$$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot h_1^T$$



* **计算损失对$b_2$的导数**：$$\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial z_2}$$



#### 3.3第一个隐藏层到输入层：

* **损失传播到第一层的输出$h_1$**：$$\frac{\partial L}{\partial h_1} = W_2^T \cdot \frac{\partial L}{\partial z_2}$$



* **计算 ReLU 激活函数的导数**：$$\frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial h_1} \cdot \text{ReLU}'(z_1)$$



* **计算损失对$h_1$的导数**：$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial z_1} \cdot x^T$$



* **计算损失对$b_1$的导数**：$$\frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial z_1}$$



### 4. 权重更新

使用梯度下降算法或 Adam 优化器来更新权重。

**更新公式**：$$W = W - \eta \cdot \frac{\partial L}{\partial W}$$



* $W$是权重矩阵，$η$是学习率，$\frac{\partial L}{\partial W}$ 是损失函数关于权重的梯度。

权重更新步骤在每一层执行：

* 更新 $W_1$,$W_2$,$W_3$和对应的偏置项$b_1$,$b_2$,$b_3$。

### 5. 优化器（Adam）介绍

Adam 优化器通过结合动量和自适应学习率进行参数更新。详细的更新公式在上面的回答中已经给出。

**1.一阶动量估计:**
计算当前梯度$\nabla_{\theta}L$的加权平均，用来估计梯度的期望。这个一阶动量主要是累积之前的梯度，使得更新方向更加平滑。

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_{\theta}L$$
$\beta_1$是一阶动量的衰减率，通常取值为 0.9。
$m_t$是当前的动量（梯度的指数加权平均）。

**2.二阶矩估计：**
计算当前梯度平方的加权平均，估计梯度的方差，用来调节学习率，避免更新步长过大。
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_{\theta}L)^2$$
$\beta_2$是二阶动量的衰减率，通常取值为 0.999。
$v_t$是梯度平方的指数加权平均。

**3.偏差修正：**
由于$\hat{m}_t$和$\hat{v}_t$在前几步可能会有较大的偏差，Adam 引入了偏差修正，减少估计的偏差。
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
$\hat{m}_t$和$\hat{v}_t$是偏差修正后的动量和二阶矩估计.


**4.参数更新：**
使用修正后的动量和方差来更新参数。Adam 的更新方式是自适应的，能根据梯度的历史动态调整学习率。
$$W_{t+1} = W_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$
$η$是学习率，通常取值在 0.001 左右。
$𝜖$是一个小的平滑项，避免除以零，通常为$10^{-8}$ 。

---
### **Reference:**

1. [TensorFlow Documentation](https://www.tensorflow.org/guide)
2. [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-1/)
3. [Adam Optimizer Paper](https://arxiv.org/abs/1412.6980)
4. [Gradient Descent and Backpropagation Overview](http://neuralnetworksanddeeplearning.com/chap2.html)
5. https://www.deeplearningbook.org/
6. http://cs231n.github.io/optimization-2/
------

